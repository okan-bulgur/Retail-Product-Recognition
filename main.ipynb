{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okan Bulgur (20200702017)\n",
        "Berke Berkay Tek√ße (20200702012)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3bA3ixHyXYo"
      },
      "source": [
        "# Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqID4LuY6cMS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from torch.utils.data import ConcatDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbrtqRc1xgLJ"
      },
      "source": [
        "#Import Data Dictionary to Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRt_-bvXtNsb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "zip_file_path = '/content/drive/MyDrive/Train.zip'\n",
        "extracted_folder_path = '/content'\n",
        "\n",
        "!unzip -q \"{zip_file_path}\" -d \"{extracted_folder_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-V9DH-WSzQr"
      },
      "outputs": [],
      "source": [
        "zip_file_path = '/content/drive/MyDrive/Test.zip'\n",
        "extracted_folder_path = '/content'\n",
        "\n",
        "!unzip -q \"{zip_file_path}\" -d \"{extracted_folder_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73W4zM4D6NYU"
      },
      "outputs": [],
      "source": [
        "zip_file_path = '/content/Train.zip'\n",
        "extracted_folder_path = '/content'\n",
        "\n",
        "!unzip -q \"{zip_file_path}\" -d \"{extracted_folder_path}\"\n",
        "\n",
        "zip_file_path = '/content/Test.zip'\n",
        "extracted_folder_path = '/content'\n",
        "\n",
        "!unzip -q \"{zip_file_path}\" -d \"{extracted_folder_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgJ4dNuhztsw"
      },
      "source": [
        "# Setted Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fZR6i_NztR6"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "IMG_SIZE = 128\n",
        "\n",
        "TRAIN_PERCENTAGE = 0.7\n",
        "VALIDATION_PERCENTAGE = 0.15\n",
        "TEST_PERCENTAGE = 0.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQELIWA1xsLc"
      },
      "source": [
        "# Select GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRr-HpNWgBUb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utZB7dWJx-5G"
      },
      "source": [
        "# Generate Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-tGr-XKPbJd"
      },
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(\n",
        "            [folder for folder in os.listdir(data_dir) if not folder.startswith(\".\")]\n",
        "        )\n",
        "        self.data = []\n",
        "\n",
        "        for label, class_name in enumerate(self.classes):\n",
        "            class_path = os.path.join(data_dir, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                for file_name in os.listdir(class_path):\n",
        "                    file_path = os.path.join(class_path, file_name)\n",
        "                    if file_name.lower().endswith(('.png', '.jpg', '.jpeg')) and not file_name.startswith(\".\"):\n",
        "                        self.data.append((file_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.data[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNcQxRFwyNiZ"
      },
      "source": [
        "# Generate Convolution Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pXoCJkCvISb"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional Layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # Output: 128x128x32\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 64x64x32\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # Output: 64x64x64\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 32x32x64\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # Output: 32x32x128\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 16x16x128\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # Output: 16x16x256\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 8x8x256\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(8 * 8 * 256, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, num_classes)\n",
        "\n",
        "        # Regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Convolutional Layers\n",
        "        x = self.pool1(nn.ReLU()(self.conv1(x)))  # Output: 64x64x32\n",
        "        x = self.pool2(nn.ReLU()(self.conv2(x)))  # Output: 32x32x64\n",
        "        x = self.pool3(nn.ReLU()(self.conv3(x)))  # Output: 16x16x128\n",
        "        x = self.pool4(nn.ReLU()(self.conv4(x)))  # Output: 8x8x256\n",
        "\n",
        "        # Flatten\n",
        "        x = torch.flatten(x, 1) # Output: 8*8*256 = 16384\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = nn.ReLU()(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ6BJ93wyhTa"
      },
      "source": [
        "# Setted Transform to Resize Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJpcoMMyjog3"
      },
      "outputs": [],
      "source": [
        "transform_1 = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLwG8xm8n9ud"
      },
      "outputs": [],
      "source": [
        "transform_2 = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomResizedCrop(size=IMG_SIZE, scale=(0.9, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG4Z5Z8HyqM2"
      },
      "source": [
        "# Generate Dataset with Resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSnxnJayjGiP"
      },
      "outputs": [],
      "source": [
        "dataset_path = f\"{extracted_folder_path}/Train\"\n",
        "dataset_1 = Dataset(data_dir=dataset_path, transform=transform_1)\n",
        "dataset_2 = Dataset(data_dir=dataset_path, transform=transform_2)\n",
        "dataset = ConcatDataset([dataset_1, dataset_2])\n",
        "\n",
        "NUM_CLASSES = len(dataset_1.classes)\n",
        "\n",
        "print(\"Classes:\")\n",
        "print(dataset_1.classes)\n",
        "print(\"Classes Size: \", NUM_CLASSES)\n",
        "print(\"Total Size: \", len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTCe49wVyxhd"
      },
      "source": [
        "# Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R70cEEkvj0Bn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(TRAIN_PERCENTAGE * dataset_size)\n",
        "val_size = int(VALIDATION_PERCENTAGE * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print(f\"Total Size: {len(dataset)}\\nTrain Size: {len(train_dataset)}\\nValidation Size: {len(val_dataset)}\\nTest Size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7aLNARI096w"
      },
      "source": [
        "# Generate Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAILZ8x5gTDt"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5QTxLOxAHj9"
      },
      "outputs": [],
      "source": [
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exiCbCer5eqC"
      },
      "source": [
        "# Calculate Weight for Whole Class (For classes with few samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qJ1ZEy8E5n6H"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_labels = [label for _, label in dataset]\n",
        "\n",
        "class_counts = Counter(all_labels)\n",
        "\n",
        "num_classes = len(dataset_1.classes)\n",
        "class_weights = torch.zeros(num_classes, dtype=torch.float)\n",
        "\n",
        "for cls, count in class_counts.items():\n",
        "    label_name = dataset_1.classes[cls]\n",
        "    print(cls, \") \", label_name, \" : \", count, \" | \", 1.0/count)\n",
        "    class_weights[cls] = 1.0 / count\n",
        "\n",
        "print(\"Class weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p60XlLce1Rv4"
      },
      "source": [
        "# Generate Model, Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34LjSgMpTstK"
      },
      "outputs": [],
      "source": [
        "model = Model(NUM_CLASSES)\n",
        "class_weights = class_weights.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNqtmawW1w-6"
      },
      "source": [
        "# Move Model to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXWdxwKf5_a7"
      },
      "outputs": [],
      "source": [
        "print(f\"Using device: {device}\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7QrVwLqMpfV"
      },
      "source": [
        "# Optimize Model Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGGwIVp-C7aL"
      },
      "outputs": [],
      "source": [
        "def optimize_model(mdl, loader):\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    mdl.train()\n",
        "\n",
        "    for i, data in enumerate(loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mdl(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss, correct_train, total_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYAMbUVZE4Vi"
      },
      "source": [
        "# Evaluate Mode Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Zbza3_DTFa"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(mdl, loader):\n",
        "    running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    mdl.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data in loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = mdl(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_val += labels.size(0)\n",
        "        correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss, correct_val, total_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkRaCcUgE8JL"
      },
      "source": [
        "# Get Optimization and Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqq3dBLBECHa"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k78W6ruPHYYJ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    running_loss, correct_train, total_train = optimize_model(model, train_loader)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracy = correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy * 100)\n",
        "\n",
        "    runing_loss, correct_val, total_val = evaluate_model(model, val_loader)\n",
        "\n",
        "    val_loss = runing_loss / len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracy = correct_val / total_val\n",
        "    val_accuracies.append(val_accuracy * 100)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], \"\n",
        "        f\"Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}, \"\n",
        "        f\"Train Acc: {train_accuracy:.2f}, Val Acc: {val_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1OG8UnAFcBV"
      },
      "source": [
        "Display Last Accurancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJWAyXJ7MOQ4"
      },
      "outputs": [],
      "source": [
        "print(\"Last Train Accurancy: \", train_accuracies[-1])\n",
        "print(\"Last Validation Accurancy: \", val_accuracies[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy4wYUWtFesv"
      },
      "source": [
        "Display Train & Validation Loss Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f61ow9NGIUcZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vJNIZRGFlpu"
      },
      "source": [
        "Display Train & Validation Accurancy Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU3XwjVAZ_Fz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfnby45jDNfs"
      },
      "source": [
        "# Display Extra Infos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpSxwYnYDR_Y"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in val_loader:\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    all_preds.extend(predicted.cpu().numpy())\n",
        "    all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHd3o7kYDjY5"
      },
      "source": [
        "Accurancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L88ZQ9yyDjHm"
      },
      "outputs": [],
      "source": [
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-GhDLj_DpAj"
      },
      "source": [
        "Measure Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5c8b_grADm9E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset_1.classes)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp.plot(ax=ax, cmap=\"Blues\", xticks_rotation='vertical')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqNu4uupDu5G"
      },
      "source": [
        "Show Classification Report\n",
        "\n",
        "\n",
        "*   **Macro Average:** Measure the balance of classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVf07LhsDwvN"
      },
      "outputs": [],
      "source": [
        "print(classification_report(all_labels, all_preds, target_names=dataset_1.classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfzk4u4OFqnD"
      },
      "source": [
        "# Setup Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UCX0Z3tF1tU"
      },
      "outputs": [],
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBvIx-5CHI8V"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    \"test_dataset\": test_dataset\n",
        "}, \"test_dataset.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Sa89YT2o4g"
      },
      "source": [
        "# Calculate Accurancy for Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9FJp5Njeh2Q"
      },
      "outputs": [],
      "source": [
        "running_loss, correct_test, total_test = evaluate_model(model, test_loader)\n",
        "\n",
        "test_loss = running_loss / len(test_loader)\n",
        "test_accuracy = (100 * correct_test) / total_test\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.2f}, Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AgWj9xbU89K"
      },
      "source": [
        "# Last Test Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeP4qnRahcNA"
      },
      "outputs": [],
      "source": [
        "test_dataset_path = f\"{extracted_folder_path}/Test\"\n",
        "test_dataset_last = Dataset(data_dir=test_dataset_path, transform=transform_1)\n",
        "\n",
        "NUM_CLASSES = len(test_dataset_last.classes)\n",
        "\n",
        "print(\"Classes:\")\n",
        "print(test_dataset_last.classes)\n",
        "print(\"Classes Size: \", NUM_CLASSES)\n",
        "print(\"Total Size: \", len(test_dataset_last))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3Tx3daQhdH3"
      },
      "outputs": [],
      "source": [
        "test_loader_last = DataLoader(test_dataset_last, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG0-_ZKJJN3X"
      },
      "source": [
        "Evaluate Last Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FB3-0Cmhem1"
      },
      "outputs": [],
      "source": [
        "running_loss, correct_test_last, total_test_last = evaluate_model(model, test_loader_last)\n",
        "\n",
        "test_loss_last = running_loss / len(test_loader_last)\n",
        "test_accuracy_last = (100 * correct_test_last) / total_test_last\n",
        "\n",
        "print(f\"Last Test Loss: {test_loss_last:.2f}, Last Test Accuracy: {test_accuracy_last:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT7Gn_KGIBPS"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65buGnihIDV8"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"model_complete.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7joW9ufIFtr"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JkRaCcUgE8JL",
        "bT7Gn_KGIBPS"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
